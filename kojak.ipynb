{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdb \n",
    "import os\n",
    "import pickle\n",
    "from gensim import corpora, models, similarities, matutils, interfaces, utils\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.INFO\n",
    "\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "\n",
    "DATA_PATH = (u'./pmc_data/pmc_text_files/')\n",
    "SAVE_LOCATION = './pmc_models_serialized/'\n",
    "GENRE_FOLDERS = os.listdir(DATA_PATH)\n",
    "ARTICLE_FILE_PATHS = []\n",
    "ARTICLE_FILE_TITLES = []\n",
    "ARTICLE_DOCUMENT_LIST = []\n",
    "\n",
    "class PubmedCorpus(object):\n",
    "    def __init__(self, data_folder=DATA_PATH):\n",
    "        self.data_folder = data_folder\n",
    "        self.dictionary = corpora.Dictionary()\n",
    "        self.load_corpus()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for root, dirs, files in os.walk(self.data_folder):\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    doc = f.read()\n",
    "                    doc_token_gen = utils.tokenize(doc, \n",
    "                                                  lowercase=True)\n",
    "                    doc_tokenized = [i for i in doc_token_gen]\n",
    "                    yield self.dictionary.doc2bow(doc_tokenized)\n",
    "### TODO clean up redundancy    \n",
    "    def load_corpus(self):\n",
    "        for root, dirs, files in os.walk(self.data_folder):\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    doc = f.read()\n",
    "                    doc_token_gen = utils.tokenize(doc, \n",
    "                                                  lowercase=True)\n",
    "                    doc_tokenized = [i for i in doc_token_gen]\n",
    "                    self.dictionary.add_documents([doc_tokenized])\n",
    "\n",
    "\n",
    "def to_unicode_or_bust(\n",
    "        obj, encoding='utf-8'):\n",
    "    if isinstance(obj, basestring):\n",
    "        if not isinstance(obj, unicode):\n",
    "            obj = unicode(obj, encoding)\n",
    "    return obj\n",
    "              \n",
    "pubmed_corpus = PubmedCorpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'distributed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-43be9b7e80b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpubmed_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpubmed_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpubmed_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_LOCATION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pubmed_tfidf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'distributed'"
     ]
    }
   ],
   "source": [
    "pubmed_tfidf = models.TfidfModel(pubmed_corpus, normalize=True)\n",
    "pubmed_tfidf.save(os.path.join(SAVE_LOCATION, 'pubmed_tfidf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform incoming vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow]) # step 2 -- use the model to transform vectors\n",
    "[(0, 0.70710678), (1, 0.70710678)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform the entire corpus (lazy, only does so when run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "...     print(doc)\n",
    "[(0, 0.57735026918962573), (1, 0.57735026918962573), (2, 0.57735026918962573)]\n",
    "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.32448702061385548), (6, 0.44424552527467476), (7, 0.32448702061385548)]\n",
    "[(2, 0.5710059809418182), (5, 0.41707573620227772), (7, 0.41707573620227772), (8, 0.5710059809418182)]\n",
    "[(1, 0.49182558987264147), (5, 0.71848116070837686), (8, 0.49182558987264147)]\n",
    "[(3, 0.62825804686700459), (6, 0.62825804686700459), (7, 0.45889394536615247)]\n",
    "[(9, 1.0)]\n",
    "[(9, 0.70710678118654746), (10, 0.70710678118654746)]\n",
    "[(9, 0.50804290089167492), (10, 0.50804290089167492), (11, 0.69554641952003704)]\n",
    "[(4, 0.62825804686700459), (10, 0.45889394536615247), (11, 0.62825804686700459)]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/gensim-0.12.4-py2.7-macosx-10.11-x86_64.egg/gensim/interfaces.py:60: UserWarning: corpus.save() stores only the (tiny) iteration object; to serialize the actual corpus content, use e.g. MmCorpus.serialize(corpus)\n",
      "  warnings.warn(\"corpus.save() stores only the (tiny) iteration object; \"\n"
     ]
    }
   ],
   "source": [
    "pubmed_corpus_tfidf = pubmed_tfidf[pubmed_corpus]\n",
    "pubmed_corpus_tfidf.save(os.path.join(SAVE_LOCATION, 'pubmed_corpus_tfidf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "failed to initialize distributed LSI (No module named Pyro4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-990dc651e07c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                              \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpubmed_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                              \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                             distributed=True)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpubmed_corpus_lsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpubmed_lsi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpubmed_corpus_tfidf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpubmed_lsi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_LOCATION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pubmed_lsi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/gensim-0.12.4-py2.7-macosx-10.11-x86_64.egg/gensim/models/lsimodel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, chunksize, decay, distributed, onepass, power_iters, extra_samples)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0;31m# distributed version was specifically requested, so this is an error state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"failed to initialize distributed LSI (%s)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"failed to initialize distributed LSI (%s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: failed to initialize distributed LSI (No module named Pyro4)"
     ]
    }
   ],
   "source": [
    "pubmed_lsi = models.LsiModel(pubmed_corpus_tfidf, \n",
    "                             id2word=pubmed_corpus.dictionary, \n",
    "                             num_topics=200)\n",
    "pubmed_corpus_lsi = pubmed_lsi[pubmed_corpus_tfidf]\n",
    "pubmed_lsi.save(os.path.join(SAVE_LOCATION, 'pubmed_lsi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look at some of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.175*\"obesity\" + 0.171*\"overweight\" + 0.168*\"bmi\" + 0.106*\"maternal\" + 0.094*\"pregnancy\" + 0.093*\"doi\" + 0.092*\"hiv\" + 0.089*\"weight\" + 0.085*\"conflict\" + 0.082*\"mortality\"'),\n",
       " (1,\n",
       "  u'-0.355*\"obesity\" + -0.353*\"bmi\" + -0.336*\"overweight\" + 0.141*\"conflict\" + -0.138*\"obese\" + -0.133*\"waist\" + -0.131*\"finnmark\" + -0.103*\"mufi\" + 0.101*\"medicines\" + 0.095*\"hiv\"')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed_lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fold in new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_doc = 'hello there overweight circumference pregnancy'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vec = pubmed_corpus.dictionary.doc2bow(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_tfidf_vec = pubmed_tfidf[new_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class PubmedCorpus(corpora.TextCorpus):\n",
    "    \n",
    "#     def get_texts(self):\n",
    "#         directory = os.walk(self.input)\n",
    "#         for root, dirs, files in directory:\n",
    "#             for file_name in files:\n",
    "#                 file_path = os.path.join(root, file_name)\n",
    "#                 with open(file_path, 'rb') as f:\n",
    "#                     file_string = to_unicode_or_bust(f.read().lower())\n",
    "#                     file_tokenized = word_tokenize(file_string)\n",
    "#                     file_no_stops = [word for word in file_tokenized if word not in STOP_WORDS]\n",
    "#                     yield file_no_stops\n",
    "\n",
    "# class PubmedCorpus(corpora.TextCorpus):\n",
    "        \n",
    "#     def get_texts(self):\n",
    "#         for file_path in self.input:\n",
    "#             with open(file_path, 'rb') as f:\n",
    "#                 file_string = to_unicode_or_bust(f.read().lower())\n",
    "#                 file_tokenized = word_tokenize(file_string)\n",
    "#                 file_no_stops = [word for word in file_tokenized if word not in STOP_WORDS]\n",
    "#                 yield file_no_stops\n",
    "                    \n",
    "                \n",
    "# def load_article_paths():\n",
    "#     for root, dirs, files in os.walk('./pmc_data/pmc_text_files/'):\n",
    "#         for name in files:\n",
    "#             ARTICLE_FILE_PATHS.append(os.path.join(root, name))\n",
    "#     genre_folders_left = len(GENRE_FOLDERS)\n",
    "#     completed_genre_folders = 0 \n",
    "#     for genre_folder in GENRE_FOLDERS:\n",
    "#         completed_genre_folders += 1\n",
    "#         genre_folder_path = os.path.join(DATA_PATH, genre_folder)\n",
    "#         genre_file_list = os.listdir(genre_folder_path)\n",
    "#         for article_file_title in genre_file_list:\n",
    "#             article_file_path = os.path.join(genre_folder_path, article_file_title)\n",
    "#             if os.path.isfile(article_file_path):\n",
    "#                 ARTICLE_FILE_TITLES.append(article_file_title)\n",
    "#                 ARTICLE_FILE_PATHS.append(article_file_path)\n",
    "# #                 with open(article_file_path, 'rb') as f:\n",
    "# #                     document = f.read()\n",
    "# #                     ARTICLE_DOCUMENT_LIST.append(document)\n",
    "\n",
    "#                 print \"done with: \", article_file_title\n",
    "#                 print \"progress: \", completed_genre_folders / float(genre_folders_left)\n",
    "#             else:\n",
    "#                 sub_article_folder_list = os.listdir(article_file_path)\n",
    "#                 for sub_article_file_title in sub_article_folder_list:\n",
    "#                     sub_article_file_path = os.path.join(article_file_path, \n",
    "#                             sub_article_file_title)\n",
    "#                     ARTICLE_FILE_TITLES.append(sub_article_file_title)\n",
    "#                     ARTICLE_FILE_PATHS.append(sub_article_file_path)\n",
    "# #                     with open(sub_article_file_path, 'rb') as f:\n",
    "# #                         document = f.read()\n",
    "# #                         ARTICLE_DOCUMENT_LIST.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pubmed Corpus from corpora.TextCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class PubmedCorpus(corpora.TextCorpus):\n",
    "#     def __init__(self, input=None, data_folder=DATA_PATH):\n",
    "#         super(TextCorpus, self).__init__()\n",
    "#         self.input = None\n",
    "#         self.data_folder = os.walk(data_folder)\n",
    "#         self.dictionary = Dictionary()\n",
    "#         self.metadata = False        \n",
    "#         if data_folder is not None:\n",
    "#             self.dictionary.add_documents\n",
    "    \n",
    "#     def __iter__(self):\n",
    "        \n",
    "        \n",
    "#     def get_texts(self, file_name):\n",
    "#         with open(file_name, 'rb') as doc_file:\n",
    "#             doc = doc_file.read()\n",
    "#             doc_tokenized = utils.tokenize(doc, lowercase=True)\n",
    "#             doc_tokenized = [word for word in doc_tokenized if word not in STOP_WORDS]\n",
    "#             yield doc_tokenized\n",
    "\n",
    "                    \n",
    "# #         for doc_file in self.input:\n",
    "# #             with open(doc_file, 'rb') as doc:\n",
    "# #                 doc_text = doc.read()\n",
    "# #                 tokenized_text = [token for token in utils.tokenize(doc_text, \n",
    "# #                                                                     lowercase=True)]\n",
    "# #                 tokenized_text = [word for word in tokenized_text if word not in STOP_WORDS]\n",
    "# #                 self.dictionary.add_documents([tokenized_text])\n",
    "# #                 yield self.dictionary.doc2bow(tokenized_text, allow_update=False)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# #         directory = os.walk(self.input)\n",
    "# #         for root, dirs, files in directory:\n",
    "# #             for file_name in files:\n",
    "# #                 file_path = os.path.join(root, file_name)\n",
    "# #                 with open(file_path, 'rb') as f:\n",
    "# #                     file_string = to_unicode_or_bust(f.read().lower())\n",
    "# #                     file_tokenized = word_tokenize(file_string)\n",
    "# #                     file_no_stops = [word for word in file_tokenized if word not in STOP_WORDS]\n",
    "# #                     yield file_no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
