{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tfidf_pm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubmed_corpus = PubmedCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubmed_corpus.load_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pubmed_corpus.document_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubmed_corpus.dictionary.doc2bow(['yo', 'yellow', 'circumference', 'obesity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save pubmed_corpus.dictionary\n",
    "    - pubmed_corpus.dictionary.save()\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubmed_tfidf = models.TfidfModel(pubmed_corpus, normalize=True)\n",
    "pubmed_tfidf.save(os.path.join(SAVE_LOCATION, 'pubmed_tfidf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform incoming vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow]) # step 2 -- use the model to transform vectors\n",
    "[(0, 0.70710678), (1, 0.70710678)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform the entire corpus (lazy, only does so when run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "...     print(doc)\n",
    "[(0, 0.57735026918962573), (1, 0.57735026918962573), (2, 0.57735026918962573)]\n",
    "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.32448702061385548), (6, 0.44424552527467476), (7, 0.32448702061385548)]\n",
    "[(2, 0.5710059809418182), (5, 0.41707573620227772), (7, 0.41707573620227772), (8, 0.5710059809418182)]\n",
    "[(1, 0.49182558987264147), (5, 0.71848116070837686), (8, 0.49182558987264147)]\n",
    "[(3, 0.62825804686700459), (6, 0.62825804686700459), (7, 0.45889394536615247)]\n",
    "[(9, 1.0)]\n",
    "[(9, 0.70710678118654746), (10, 0.70710678118654746)]\n",
    "[(9, 0.50804290089167492), (10, 0.50804290089167492), (11, 0.69554641952003704)]\n",
    "[(4, 0.62825804686700459), (10, 0.45889394536615247), (11, 0.62825804686700459)]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubmed_corpus_tfidf = pubmed_tfidf[pubmed_corpus]\n",
    "pubmed_corpus_tfidf.save(os.path.join(SAVE_LOCATION, 'pubmed_corpus_tfidf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pubmed_corpus.dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??utils.chunkize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubmed_lsi = models.LsiModel(pubmed_corpus_tfidf, \n",
    "                             id2word=pubmed_corpus.dictionary, \n",
    "                             num_topics=200)\n",
    "pubmed_corpus_lsi = pubmed_lsi[pubmed_corpus_tfidf]\n",
    "pubmed_lsi.save(os.path.join(SAVE_LOCATION, 'pubmed_lsi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look at some of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubmed_lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folding in a new document\n",
    "\n",
    "tfidf = models.tfidfmodel.TfidfModel.load('tfidf')\n",
    "\n",
    "lsi = models.LsiModel.load('lsi')\n",
    "\n",
    "Create an index transformer that calculates similarity based on our space\n",
    " \n",
    "test_index1 = similarities.MatrixSimilarity(test_lsi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pubmed_corpus.dictionary.load('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_doc = 'hello there overweight circumference pregnancy'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vec = pubmed_corpus.dictionary.doc2bow(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_tfidf_vec = pubmed_tfidf[new_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary.doc2bow(['hello', 'something'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class thing:\n",
    "    def __init__(self):\n",
    "        self.array = [(1, 2, 3), (3,3, 3) , (2, 2, 2)]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i, j, k in self.array:\n",
    "            yield i * 2\n",
    "            \n",
    "    def load_stuff(self):\n",
    "        for i in self:\n",
    "            print i\n",
    "            \n",
    "    def again(self):\n",
    "        for i in self:\n",
    "            print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = thing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.load_stuff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class PubmedCorpus(corpora.TextCorpus):\n",
    "    \n",
    "#     def get_texts(self):\n",
    "#         directory = os.walk(self.input)\n",
    "#         for root, dirs, files in directory:\n",
    "#             for file_name in files:\n",
    "#                 file_path = os.path.join(root, file_name)\n",
    "#                 with open(file_path, 'rb') as f:\n",
    "#                     file_string = to_unicode_or_bust(f.read().lower())\n",
    "#                     file_tokenized = word_tokenize(file_string)\n",
    "#                     file_no_stops = [word for word in file_tokenized if word not in STOP_WORDS]\n",
    "#                     yield file_no_stops\n",
    "\n",
    "# class PubmedCorpus(corpora.TextCorpus):\n",
    "        \n",
    "#     def get_texts(self):\n",
    "#         for file_path in self.input:\n",
    "#             with open(file_path, 'rb') as f:\n",
    "#                 file_string = to_unicode_or_bust(f.read().lower())\n",
    "#                 file_tokenized = word_tokenize(file_string)\n",
    "#                 file_no_stops = [word for word in file_tokenized if word not in STOP_WORDS]\n",
    "#                 yield file_no_stops\n",
    "                    \n",
    "                \n",
    "# def load_article_paths():\n",
    "#     for root, dirs, files in os.walk('./pmc_data/pmc_text_files/'):\n",
    "#         for name in files:\n",
    "#             ARTICLE_FILE_PATHS.append(os.path.join(root, name))\n",
    "#     genre_folders_left = len(GENRE_FOLDERS)\n",
    "#     completed_genre_folders = 0 \n",
    "#     for genre_folder in GENRE_FOLDERS:\n",
    "#         completed_genre_folders += 1\n",
    "#         genre_folder_path = os.path.join(DATA_PATH, genre_folder)\n",
    "#         genre_file_list = os.listdir(genre_folder_path)\n",
    "#         for article_file_title in genre_file_list:\n",
    "#             article_file_path = os.path.join(genre_folder_path, article_file_title)\n",
    "#             if os.path.isfile(article_file_path):\n",
    "#                 ARTICLE_FILE_TITLES.append(article_file_title)\n",
    "#                 ARTICLE_FILE_PATHS.append(article_file_path)\n",
    "# #                 with open(article_file_path, 'rb') as f:\n",
    "# #                     document = f.read()\n",
    "# #                     ARTICLE_DOCUMENT_LIST.append(document)\n",
    "\n",
    "#                 print \"done with: \", article_file_title\n",
    "#                 print \"progress: \", completed_genre_folders / float(genre_folders_left)\n",
    "#             else:\n",
    "#                 sub_article_folder_list = os.listdir(article_file_path)\n",
    "#                 for sub_article_file_title in sub_article_folder_list:\n",
    "#                     sub_article_file_path = os.path.join(article_file_path, \n",
    "#                             sub_article_file_title)\n",
    "#                     ARTICLE_FILE_TITLES.append(sub_article_file_title)\n",
    "#                     ARTICLE_FILE_PATHS.append(sub_article_file_path)\n",
    "# #                     with open(sub_article_file_path, 'rb') as f:\n",
    "# #                         document = f.read()\n",
    "# #                         ARTICLE_DOCUMENT_LIST.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pubmed Corpus from corpora.TextCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class PubmedCorpus(corpora.TextCorpus):\n",
    "#     def __init__(self, input=None, data_folder=DATA_PATH):\n",
    "#         super(TextCorpus, self).__init__()\n",
    "#         self.input = None\n",
    "#         self.data_folder = os.walk(data_folder)\n",
    "#         self.dictionary = Dictionary()\n",
    "#         self.metadata = False        \n",
    "#         if data_folder is not None:\n",
    "#             self.dictionary.add_documents\n",
    "    \n",
    "#     def __iter__(self):\n",
    "        \n",
    "        \n",
    "#     def get_texts(self, file_name):\n",
    "#         with open(file_name, 'rb') as doc_file:\n",
    "#             doc = doc_file.read()\n",
    "#             doc_tokenized = utils.tokenize(doc, lowercase=True)\n",
    "#             doc_tokenized = [word for word in doc_tokenized if word not in STOP_WORDS]\n",
    "#             yield doc_tokenized\n",
    "\n",
    "                    \n",
    "# #         for doc_file in self.input:\n",
    "# #             with open(doc_file, 'rb') as doc:\n",
    "# #                 doc_text = doc.read()\n",
    "# #                 tokenized_text = [token for token in utils.tokenize(doc_text, \n",
    "# #                                                                     lowercase=True)]\n",
    "# #                 tokenized_text = [word for word in tokenized_text if word not in STOP_WORDS]\n",
    "# #                 self.dictionary.add_documents([tokenized_text])\n",
    "# #                 yield self.dictionary.doc2bow(tokenized_text, allow_update=False)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# #         directory = os.walk(self.input)\n",
    "# #         for root, dirs, files in directory:\n",
    "# #             for file_name in files:\n",
    "# #                 file_path = os.path.join(root, file_name)\n",
    "# #                 with open(file_path, 'rb') as f:\n",
    "# #                     file_string = to_unicode_or_bust(f.read().lower())\n",
    "# #                     file_tokenized = word_tokenize(file_string)\n",
    "# #                     file_no_stops = [word for word in file_tokenized if word not in STOP_WORDS]\n",
    "# #                     yield file_no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
