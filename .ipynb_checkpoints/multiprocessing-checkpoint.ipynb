{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "??multiprocessing.pool.Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process InputQueue-150:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python2.7/site-packages/gensim-0.12.4-py2.7-macosx-10.11-x86_64.egg/gensim/utils.py\", line 823, in run\n",
      "    wrapped_chunk = [list(chunk)]\n",
      "  File \"<ipython-input-141-382034a00247>\", line 58, in token_iter\n",
      "    for files in utils.chunkize(self.file_path_iter(), chunksize=10 * pool_size, maxsize=2):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/gensim-0.12.4-py2.7-macosx-10.11-x86_64.egg/gensim/utils.py\", line 874, in chunkize\n",
      "    worker.start()\n",
      "  File \"/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 124, in start\n",
      "    'daemonic processes are not allowed to have children'\n",
      "AssertionError: daemonic processes are not allowed to have children\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-382034a00247>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mpubmed_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPubmedCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-141-382034a00247>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_folder)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#     def __iter__(self):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-382034a00247>\u001b[0m in \u001b[0;36mload_corpus\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunkize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/gensim-0.12.4-py2.7-macosx-10.11-x86_64.egg/gensim/utils.pyc\u001b[0m in \u001b[0;36mchunkize\u001b[0;34m(corpus, chunksize, maxsize, as_numpy)\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m                 \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/queues.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pdb \n",
    "import os\n",
    "import pickle\n",
    "from gensim import corpora, models, similarities, matutils, interfaces, utils\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import logging\n",
    "import multiprocessing\n",
    "pool_size = multiprocessing.cpu_count() - 1\n",
    "\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "\n",
    "DATA_PATH = (u'./pmc_data/pmc_text_files/')\n",
    "SAVE_LOCATION = './pmc_models_serialized/'\n",
    "GENRE_FOLDERS = os.listdir(DATA_PATH)\n",
    "ARTICLE_FILE_PATHS = []\n",
    "ARTICLE_FILE_TITLES = []\n",
    "ARTICLE_DOCUMENT_LIST = []\n",
    "\n",
    "class PubmedCorpus(object):\n",
    "    def __init__(self, data_folder=DATA_PATH):\n",
    "        self.data_folder = data_folder\n",
    "        self.dictionary = corpora.Dictionary()\n",
    "        self.load_corpus()\n",
    "        \n",
    "#     def __iter__(self):\n",
    "#         for root, dirs, files in os.walk(self.data_folder):\n",
    "#             for file_name in files:\n",
    "#                 file_path = os.path.join(root, file_name)\n",
    "#                 with open(file_path, 'rb') as f:\n",
    "#                     doc = f.read()\n",
    "#                     doc_token_gen = utils.tokenize(doc, \n",
    "#                                                   lowercase=True)\n",
    "#                     doc_tokenized = [i for i in doc_token_gen]\n",
    "#                     yield doc_tokenized\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for root, dirs, files in os.walk(self.data_folder):\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    doc = f.read()\n",
    "                    doc_token_gen = utils.tokenize(doc, lowercase=True)\n",
    "                    doc_tokenized = [i for i in doc_token_gen if i not in STOP_WORDS]\n",
    "                    yield self.dictionary.doc2bow(doc_tokenized)\n",
    "### TODO clean up redundancy    \n",
    "\n",
    "    def load_corpus(self):\n",
    "        for group in utils.chunkize(self.token_iter(), chunksize=10 , maxsize=1):\n",
    "            self.dictionary.add_documents(group)            \n",
    "\n",
    "#         for doc_tokenized in self.token_iter():\n",
    "#             self.dictionary.add_documents([doc_tokenized])\n",
    "            \n",
    "    def token_iter(self):\n",
    "        pool = multiprocessing.Pool(pool_size)\n",
    "        for files in utils.chunkize(self.file_path_iter(), chunksize=10 * pool_size, maxsize=2):\n",
    "            for result in pool.imap(self.tokenized_from_file, files):  # chunksize=10):\n",
    "                yield result\n",
    "        pool.terminate()        \n",
    "        \n",
    "#         for file_path in self.file_path_iter():\n",
    "#             tokenized_file = self.tokenized_from_file(file_path)\n",
    "#             yield tokenized_file\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def token_iter(self):\n",
    "#         for root, dirs, files in os.walk(self.data_folder): \n",
    "#             for file_name in files:\n",
    "#                 file_path = os.path.join(root, file_name)\n",
    "#                 tokenized_file = self.tokenized_from_file(file_path)\n",
    "#                 yield tokenized_file\n",
    "\n",
    "#         for root, dirs, files in os.walk(self.data_folder): \n",
    "#             for file_name in files:\n",
    "#                 file_path = os.path.join(root, file_name)\n",
    "#                 with open(file_path, 'rb') as f:\n",
    "#                     doc = f.read()\n",
    "#                     doc_token_gen = utils.tokenize(doc, lowercase=True)\n",
    "#                     doc_tokenized = [i for i in doc_token_gen if i not in STOP_WORDS]\n",
    "#                     return [doc_tokenized]  \n",
    "\n",
    "    def file_path_iter(self):\n",
    "        for root, dirs, files in os.walk(self.data_folder):\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                yield file_path\n",
    "                \n",
    "    def tokenized_from_file(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            doc = f.read()\n",
    "            doc_token_gen = utils.tokenize(doc, lowercase=True)\n",
    "            doc_tokenized = [i for i in doc_token_gen if i not in STOP_WORDS]\n",
    "            return doc_tokenized\n",
    "        \n",
    "        \n",
    "def to_unicode_or_bust(\n",
    "        obj, encoding='utf-8'):\n",
    "    if isinstance(obj, basestring):\n",
    "        if not isinstance(obj, unicode):\n",
    "            obj = unicode(obj, encoding)\n",
    "    return obj\n",
    "              \n",
    "pubmed_corpus = PubmedCorpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??pool.imap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(49, 1), (561, 1)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed_corpus.dictionary.doc2bow(['yo', 'yellow', 'circumference', 'obesity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "pool_size = 2\n",
    "big_list = range(1000)\n",
    "result_list = []\n",
    "def splitter(x):\n",
    "    return x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "result_list = []\n",
    "\n",
    "\n",
    "\n",
    "def processor():\n",
    "    with open('./pmc_data/pmc_text_files/Arch_Dis_Child/Arch_Dis_Child_2007_Apr_11_92(4)_298-303.txt') as f:\n",
    "        pool = multiprocessing.Pool(pool_size)\n",
    "        for group in utils.chunkize(f.readlines(), chunksize=10 * pool_size, maxsize=2):\n",
    "            for result in pool.imap(splitter, group):  # chunksize=10):\n",
    "                yield result\n",
    "        pool.terminate()\n",
    "\n",
    "for i in processor():\n",
    "    result_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "[4, 5, 6, 7]\n",
      "[8, 9]\n"
     ]
    }
   ],
   "source": [
    "for i in utils.chunkize(range(10), chunksize=4, maxsize=2):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubmed_tfidf = models.TfidfModel(pubmed_corpus, normalize=True)\n",
    "pubmed_tfidf.save(os.path.join(SAVE_LOCATION, 'pubmed_tfidf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform incoming vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow]) # step 2 -- use the model to transform vectors\n",
    "[(0, 0.70710678), (1, 0.70710678)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform the entire corpus (lazy, only does so when run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "...     print(doc)\n",
    "[(0, 0.57735026918962573), (1, 0.57735026918962573), (2, 0.57735026918962573)]\n",
    "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.32448702061385548), (6, 0.44424552527467476), (7, 0.32448702061385548)]\n",
    "[(2, 0.5710059809418182), (5, 0.41707573620227772), (7, 0.41707573620227772), (8, 0.5710059809418182)]\n",
    "[(1, 0.49182558987264147), (5, 0.71848116070837686), (8, 0.49182558987264147)]\n",
    "[(3, 0.62825804686700459), (6, 0.62825804686700459), (7, 0.45889394536615247)]\n",
    "[(9, 1.0)]\n",
    "[(9, 0.70710678118654746), (10, 0.70710678118654746)]\n",
    "[(9, 0.50804290089167492), (10, 0.50804290089167492), (11, 0.69554641952003704)]\n",
    "[(4, 0.62825804686700459), (10, 0.45889394536615247), (11, 0.62825804686700459)]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubmed_corpus_tfidf = pubmed_tfidf[pubmed_corpus]\n",
    "pubmed_corpus_tfidf.save(os.path.join(SAVE_LOCATION, 'pubmed_corpus_tfidf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pubmed_lsi = models.LsiModel(pubmed_corpus_tfidf, \n",
    "                             id2word=pubmed_corpus.dictionary, \n",
    "                             num_topics=200)\n",
    "pubmed_corpus_lsi = pubmed_lsi[pubmed_corpus_tfidf]\n",
    "pubmed_lsi.save(os.path.join(SAVE_LOCATION, 'pubmed_lsi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### look at some of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'-0.179*\"obesity\" + -0.175*\"overweight\" + -0.171*\"bmi\" + -0.107*\"maternal\" + -0.095*\"pregnancy\" + -0.093*\"doi\" + -0.093*\"hiv\" + -0.090*\"weight\" + -0.083*\"conflict\" + -0.083*\"mortality\"'),\n",
       " (1,\n",
       "  u'0.354*\"obesity\" + 0.352*\"bmi\" + 0.335*\"overweight\" + 0.137*\"obese\" + -0.137*\"conflict\" + 0.133*\"waist\" + 0.131*\"finnmark\" + 0.103*\"mufi\" + -0.103*\"medicines\" + -0.099*\"hiv\"')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed_lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fold in new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_doc = 'hello there overweight circumference pregnancy'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vec = pubmed_corpus.dictionary.doc2bow(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_tfidf_vec = pubmed_tfidf[new_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.doc2bow(['hello', 'something'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class thing:\n",
    "    def __init__(self):\n",
    "        self.array = [(1, 2, 3), (3,3, 3) , (2, 2, 2)]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i, j, k in self.array:\n",
    "            yield i * 2\n",
    "            \n",
    "    def load_stuff(self):\n",
    "        for i in self:\n",
    "            print i\n",
    "            \n",
    "    def again(self):\n",
    "        for i in self:\n",
    "            print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = thing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "6\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "a.load_stuff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class PubmedCorpus(corpora.TextCorpus):\n",
    "    \n",
    "#     def get_texts(self):\n",
    "#         directory = os.walk(self.input)\n",
    "#         for root, dirs, files in directory:\n",
    "#             for file_name in files:\n",
    "#                 file_path = os.path.join(root, file_name)\n",
    "#                 with open(file_path, 'rb') as f:\n",
    "#                     file_string = to_unicode_or_bust(f.read().lower())\n",
    "#                     file_tokenized = word_tokenize(file_string)\n",
    "#                     file_no_stops = [word for word in file_tokenized if word not in STOP_WORDS]\n",
    "#                     yield file_no_stops\n",
    "\n",
    "# class PubmedCorpus(corpora.TextCorpus):\n",
    "        \n",
    "#     def get_texts(self):\n",
    "#         for file_path in self.input:\n",
    "#             with open(file_path, 'rb') as f:\n",
    "#                 file_string = to_unicode_or_bust(f.read().lower())\n",
    "#                 file_tokenized = word_tokenize(file_string)\n",
    "#                 file_no_stops = [word for word in file_tokenized if word not in STOP_WORDS]\n",
    "#                 yield file_no_stops\n",
    "                    \n",
    "                \n",
    "# def load_article_paths():\n",
    "#     for root, dirs, files in os.walk('./pmc_data/pmc_text_files/'):\n",
    "#         for name in files:\n",
    "#             ARTICLE_FILE_PATHS.append(os.path.join(root, name))\n",
    "#     genre_folders_left = len(GENRE_FOLDERS)\n",
    "#     completed_genre_folders = 0 \n",
    "#     for genre_folder in GENRE_FOLDERS:\n",
    "#         completed_genre_folders += 1\n",
    "#         genre_folder_path = os.path.join(DATA_PATH, genre_folder)\n",
    "#         genre_file_list = os.listdir(genre_folder_path)\n",
    "#         for article_file_title in genre_file_list:\n",
    "#             article_file_path = os.path.join(genre_folder_path, article_file_title)\n",
    "#             if os.path.isfile(article_file_path):\n",
    "#                 ARTICLE_FILE_TITLES.append(article_file_title)\n",
    "#                 ARTICLE_FILE_PATHS.append(article_file_path)\n",
    "# #                 with open(article_file_path, 'rb') as f:\n",
    "# #                     document = f.read()\n",
    "# #                     ARTICLE_DOCUMENT_LIST.append(document)\n",
    "\n",
    "#                 print \"done with: \", article_file_title\n",
    "#                 print \"progress: \", completed_genre_folders / float(genre_folders_left)\n",
    "#             else:\n",
    "#                 sub_article_folder_list = os.listdir(article_file_path)\n",
    "#                 for sub_article_file_title in sub_article_folder_list:\n",
    "#                     sub_article_file_path = os.path.join(article_file_path, \n",
    "#                             sub_article_file_title)\n",
    "#                     ARTICLE_FILE_TITLES.append(sub_article_file_title)\n",
    "#                     ARTICLE_FILE_PATHS.append(sub_article_file_path)\n",
    "# #                     with open(sub_article_file_path, 'rb') as f:\n",
    "# #                         document = f.read()\n",
    "# #                         ARTICLE_DOCUMENT_LIST.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pubmed Corpus from corpora.TextCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class PubmedCorpus(corpora.TextCorpus):\n",
    "#     def __init__(self, input=None, data_folder=DATA_PATH):\n",
    "#         super(TextCorpus, self).__init__()\n",
    "#         self.input = None\n",
    "#         self.data_folder = os.walk(data_folder)\n",
    "#         self.dictionary = Dictionary()\n",
    "#         self.metadata = False        \n",
    "#         if data_folder is not None:\n",
    "#             self.dictionary.add_documents\n",
    "    \n",
    "#     def __iter__(self):\n",
    "        \n",
    "        \n",
    "#     def get_texts(self, file_name):\n",
    "#         with open(file_name, 'rb') as doc_file:\n",
    "#             doc = doc_file.read()\n",
    "#             doc_tokenized = utils.tokenize(doc, lowercase=True)\n",
    "#             doc_tokenized = [word for word in doc_tokenized if word not in STOP_WORDS]\n",
    "#             yield doc_tokenized\n",
    "\n",
    "                    \n",
    "# #         for doc_file in self.input:\n",
    "# #             with open(doc_file, 'rb') as doc:\n",
    "# #                 doc_text = doc.read()\n",
    "# #                 tokenized_text = [token for token in utils.tokenize(doc_text, \n",
    "# #                                                                     lowercase=True)]\n",
    "# #                 tokenized_text = [word for word in tokenized_text if word not in STOP_WORDS]\n",
    "# #                 self.dictionary.add_documents([tokenized_text])\n",
    "# #                 yield self.dictionary.doc2bow(tokenized_text, allow_update=False)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# #         directory = os.walk(self.input)\n",
    "# #         for root, dirs, files in directory:\n",
    "# #             for file_name in files:\n",
    "# #                 file_path = os.path.join(root, file_name)\n",
    "# #                 with open(file_path, 'rb') as f:\n",
    "# #                     file_string = to_unicode_or_bust(f.read().lower())\n",
    "# #                     file_tokenized = word_tokenize(file_string)\n",
    "# #                     file_no_stops = [word for word in file_tokenized if word not in STOP_WORDS]\n",
    "# #                     yield file_no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
